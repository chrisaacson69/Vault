Claude Opus 4.6 just dropped and it changed the AI agent game again because 16 Claude Opus 4.6 agents just coded and set the record for the length of time that an AI agent has coded autonomously. They coded for two weeks straight. No human writing the code and they delivered a fully functional C compiler. For for reference, that is over a 100,000 lines of code in Rust. It can build the Linux kernel on three different architectures. It passes 99% of a special quote torture test suite developed for compilers. It compiles Postgress. It compiles a bunch of other things. And it cost only $20,000 to build, which sounds like a lot for you and me, but it's not a lot if you're thinking about how much human equivalent work it would cost to write a new compiler. I keep saying we're moving fast, and it's even hard for me to keep up. A year ago, autonomous AI coding could top out at barely 30 minutes before the model lost the thread. Barely 30 minutes. And now we're at 2 weeks. Just last summer, Rock 10 got 7 hours out of Claude and everybody thought it was incredible. 30 minutes to 2 weeks in 12 months. That is not a trend line. That is a phase change. The entire world is shifting. Even one of the anthropic researchers involved in the project admitted what we're all thinking. I did not expect this to be anywhere near possible so early in 2026. Opus 4.6 shipped on February 5th. It has been just over a week. And the version of cutting edge that existed in January, just a few weeks ago, that already feels like a lifetime ago. Here's how fast things are changing. Just in Anthropic's own road map, Opus 4.5, shipped in November of 2025, just a couple of months ago. It was Anthropic's most capable model at the time. It was good on reasoning, good at code, reliable against long documents. It was roughly the state-of-the-art. Just a few months later, Opus 4.6 6 shipped with a 5x expansion in the context window versus Opus 4.5. That means it went from 200,000 tokens to a million. Opus 4.6 shipped with the ability to hold roughly 50,000 lines of code in a single context session in in its head, so to speak, up from 10,000 previously with Opus 4.5. That is a 4x improvement in coder document retrieval over just a couple of months. The benchmarks measures are off the charts. And you guys know I don't pay a lot of attention to benchmarks, but when you see something like nearly doubled reasoning capacity on the ARC AGI2 measure, you got to pay attention. It shows you how fast things are moving, even if you don't entirely buy the benchmark itself. And Opus 4.6 adds a new capability that did not exist at all in January. Agent teams. Multiple instances of cloud code autonomously working together as one with a lead agent coordinating the work. specialist handling subsystems and direct peer-to-peer messaging between agents. That's not a metaphor for collaboration. That is automatic actual collaboration between autonomous software agents in an enterprise system. All of this in just a couple of months. The pace of change in AI is a phrase that people keep repeating and they don't really internalize what it means. This is what it means. The tools that you mastered in January are a different generation from the tools that shipped this week. It's not a minor update, people. It is an entirely different generation. Your January mental model of what AI can and cannot do is already wrong. I was texting with a friend just this past week, and he was telling me about the Rockin results in 7 hours. And I had to tell him, I know you think you're up to date, but the record is now 2 weeks. And by the way, Rockuten using Opus 4.6 was able to have the AI manage 50 developers. That is how fast we're moving. that AI can boss 50 engineers around. Now, the 5x context window is the number anthropic put in the press release. It's the wrong number to focus on. The right number is a benchmark originally developed by OpenAI called the MRCV2 score. That sounds like a mouthful and it's used to measure something that matters enormously that nobody was testing properly. Can a model retrieve and use the information inside a long context window? In other words, can you find a needle in the haststack? It's not about whether you can quote unquote put a million tokens into the context window. Every major model can accept big context windows in January 2026. The question is whether the model can find, retrieve, and use what you put in there. That is what matters. Sonnet 4.5, which was a great model from Claude just a few months ago, does have a million token window, but the ability to find that needle in the haststack was very low. About one chance in five or 18.5%. Gemini 3 Pro a little bit better at finding that needle in the haststack across its context window about one chance in four 26.3%. These were the best available in January. They could hold your codebase. They couldn't reliably read it. The context window was like a filing cabinet with no index. Documents went in, but retrieving them was kind of a random guess past the first quarter of the content. Guess what? Guess what? Opus 4.6 six at a million tokens has a 76% chance of finding that needle in the hay stack. At 256,000 tokens or a quarter of the context window, that rises to 93%. That is the number that matters. That is why 4.6 feels like such a giant leap. It's not because of the benchmark score. It's because there's a massive difference between a model that can hold 50,000 lines of code and a model that can hold them 50,000 lines of code and know what's on every line all at the same time. This is the difference between a model that sees one file at a time and a model that holds the entire system in its head simultaneously. Every import, every dependency, every interaction between modules, all visible at once. A senior engineer working on a large codebase carries a mental model of the whole system and they know that changing the O module can break the session handler. They know the rate limiter shares state with the load balancer. It's not because they looked it up. It's because they've lived in the code long enough that the architecture becomes a matter of intuition, not a matter of documentation. That holistic awareness is often what separates a senior engineer from a contractor reading the codebase for the first time. Opus 4.6 can do this for 50,000 lines of code simultaneously. Not by summarizing, not by searching and not with years of experience. It just holds the entire context and reasons across it the way a human mind does with a system it knows very very deeply. And because working memory has improved this dramatically in the span of just a couple of months, it's actually not hard to see where the trajectory is going to go from here. The C compiler project 100,000 lines in Rust did require 16 parallel agents precisely because even a million token context window can't hold that whole project at once in its head. But at the current rate of improvement, it won't require 16 agents for long. Let me tell you more about the Rockuten story with the 50 developers. Now, Rocket is a Japanese ecom and fintech conglomerate, and they deployed clawed code across their engineering org, not as a pilot, but in production, handling real work and touching real code that ships to real users. Use Kaji, Rakuten's general manager for AI, reported what happened when they put Opus 4.6 on their issue tracker. Clawed Opus 4.6 closed 13 issues itself. It assigned 12 issues to the right team members across a team of 50 in a single day. It effectively managed a 50 person org across six separate code repositories and also knew when to escalate to a human. It wasn't that the AI helped the engineer close the tickets. I want to be clear about that. It closed issues autonomously. It did the work of an individual contributor engineer. It also routed work correctly across a 50 person org. The model understood not just the code but the org chart. Which team owns which repo? which engineer has context on which subsystem, what closes versus what needs to escalate. That's not just code intelligence, that is management intelligence. And a system that can route engineering work correctly is a system that understands organizational dependencies the way a human lead understands them. Which means the coordination function that engineering managers spend half their time on just became automatable in a couple of months. Think about the cost structure that implies. A senior engineering manager at a company like Rakuten might cost a quarter million dollars a year fully loaded, maybe more. A meaningful part of their job, ticket triage, work routing, dependency tracking, cross team coordination. That is exactly what Opus 4.6 demonstrated it could handle. Not the judgment calls about what to build next, not the career development conversation, and it wasn't done over weeks and weeks and weeks, but the fact that it can do operational coordination that typically takes 15 to 20 hours a week and demonstrated it could do it for a full day, it shows you where things are going. And the broader numbers tell the same story. It is common now to see hours and hours and hours of sustained autonomous coding for individuals who are playing with this. not in the controlled enterprise environment even people can kick off multi-hour long coding sessions and just walk away and do other things and come back and see fully working software. That is no longer an unusual thing in February 2026. And Rockwood isn't stopping here. They're building an ambient agent that breaks down complicated tasks into 24 parallel clawed code sessions. Each single one handling a different slice of their massive mono repo. A month of human engineering is generating a simultaneously running 24 agent stream that helps them to build and catch issues and that's in production. Now the detail that gets buried under these big numbers might be more interesting than all of the numbers themselves because non-technical employees at Rockoten are able to use that system to contribute to development through the cloud code terminal interface. That is right. The terminal is not just for engineers anymore. People who have never written code are able to ship features because of the work Rockin has done to integrate cloud code. So the boundary between technical and non-technical, it keeps breaking that down. The distinction that has organized knowledge worker hiring and compensation for 30 years is dissolving in a matter of months. It's not dissolving at the speed of your ability to deploy a multi-month project and is not dissolving at the speed it takes to retrain humans. That is why this is shocking. This is all happening faster than we can adjust to it. One of the features that is most hard for us to wrap our minds around is the agent teams feature that Opus 4.6 shipped. Tropic calls them team swarms internally, which is a little scary and I can see why the marketing team changed that. But the name is accurate. It's not a marketing term. It's an architecture. Multiple instances of clawed code are architected to run simultaneously. Every single one in its own context window. And they coordinate through a shared task system that has three simple states, right? Pending, in progress, and incompleted. Pending, in progress, and completed. One instance of Cloud Code is going to act as your lead developer. It will decompose the project into work items and assign them to specialists, track dependencies, and unlock bottlenecks. This is just like what Opus 4.6 did for those 50 developers. The specialist agents work independently, and when they need something, they don't just go through the lead, by the way. They can message each other directly. peer-to-peer coordination. It's not hub and spoke. There's a front-end agent, there's a back-end agent, there's a testing agent. Effectively, they are recreating the entire software engineering org inside claude code team swarms. And this is how that C compiler got built. It's not one model doing everything sequentially, right? It's 16 agents that worked in parallel. Some building the parser, some building the code generator, some building the optimizer. And they all coordinated through the same kinds of structures that existing human engineering teams use, except they work 24 hours a day. They don't have standups, and they resolve coordinations through direct messaging rather than waiting for the next sprint planning session. One of the running questions in AI has been whether agents will reinvent management. I think this argues strongly that they did. Curser's autonomous agent swarm independently organized itself into hierarchical structures and strong DM published a production framework called software factory that's built around exactly the same hierarchical pattern. And now anthropic has shipped a feature with 13 distinct operations from spawning managing to coordinating agents. This is not really coincidence. It's essentially convergent evolution in AI. Hierarchy isn't a human organizational choice imposed on systems to ma imposed on systems to maintain control. It's an emergent property of coordinating multiple intelligent agents on complicated tasks. Humans invented management because management is what intelligence does when it needs to coordinate at scale. AI agents effectively discovered the same thing because the constraints are structural. They're not cultural. You need someone to track dependencies, right? You need specialists. You need communication channels. You need a shared understanding of what has been done and what hasn't yet been done. We did not impose management on AI. AI effectively discovered management and we helped to build the structure and Opus 4.6 is the first model that ships with the infrastructure to run all of this as just another feature. On the same day Opus 4.6 launched, Enthropic published a result that got much less attention than that C compiler story, but it might matter more in the long run. They gave Opus 4.6 six basic tools, Python, debuggers, fuzzers, and they pointed it at an open-source codebase. There were no specific vulnerability hunting instructions. There were no curated targets. This wasn't a fake test. They just said, "Here's some tools. Here's some code. Can you find the problems?" It found over 500 previously unknown high severity, what's called zeroday vulnerabilities, which means fix it right now. 500 in code that had been reviewed by human security researchers scanned by existing automated tools deployed in production systems used by millions of us. Code that the security community had considered audited with when traditional fuzzing by the way fuzzing is the fancy technical word for finding bugs and making sure you check all the code. It's like fuzzing your hand through the carpet and finding a pin or something. And when manual analysis failed, using a tool called ghost script, which is what you use to check these things, the model independently decided to analyze it a different way, going directly to the project's git history. That's right. It worked around obstacles and it read through years of commit logs to understand the codebase's evolution. Nobody told it to do this. It just decided to do it. And it identified areas where security relevant changes had been made hastily or incompletely all on its own. It invented a detection methodology that no one had told it to use. It reasoned about the code's history, not just about its current state. And it used that understanding of time to find vulnerabilities that static analysis could not reach. Humans didn't do this. This is why it found the bugs it found. This is what happens when reasoning meets working memory. The model doesn't scan for known patterns the way existing tools do. It builds a mental model and I think that's the only metaphor that works at this point of how the code works, how data flows, where trust boundaries exist, where assumptions get made and where they might break down. And then it probes the weak spots with the creativity of a researcher and the patience of a machine that never gets tired of reading commit logs. And I guarantee you, human engineers get tired of that. The security implications alone would justify calling Opus 4.6 a generational release. And yet again, I remind you, it's only been a couple of months. There'll be another one in a couple of months. But this was not the headline feature of Opus 4.6. As exciting as it is, it wasn't even the second headline feature. 500 zero days was the side demonstration. That is the density of capability improvement that has been packed into a single model update shipped on a Wednesday, February. Look, there are skeptics for every release and there were skeptics for 4.6 as well. And the skepticism tracks historically. AI benchmark improvements have underdelivered before and repeatedly for years. And that is why I don't depend a lot on benchmarks. Sure enough, within hours of launch, thread started to appear on the cloud subreddit. Is Opus 4.6 labbotomized? Is it nerfed? The pattern seems to repeat with every major model release. Power users who have fine-tuned their workflows for the previous version discover the new version handle certain tasks differently. The Reddit consensus, for what it's worth, has decided that 4.6 six is better at coding and worse at writing. I haven't found that personally, but dismissing it would also probably be dishonest. Model releases involve trade-offs. Model releases often involve changes to the agent harness, which is all of the system prompting that the model makers put in that goes around the deployment that they don't talk about and they don't release. We don't know how it changed. We feel the change when we work with the system. So, I'm sure it's possible that if you were a Reddit user who was used to a special prompt pattern that worked on Opus 4.5, you might indeed be frustrated that that pattern did not work on a much more capable model overall. So, I get the skepticism. I also get that people are tired of hearing the world changing every couple of months. It is exhausting to keep up with. But that doesn't mean it's not real. And that is part of why I'm telling so many specific stories. It's important not to just look at the headlines. It's important not to look at some number changing on some stupid benchmark. It's important to hear the stories of how AI is actually changing in production now. So what does this feel like if you are not an engineer? What does this feel like if you don't write code? Cuz the C compiler, let's be honest, it's a developer story. The benchmarks are developer metrics. But the change underneath, what makes 4.6 special isn't about developers per se. It's about what happens when AI can sustain complicated work for hours and days instead of minutes. Two CNBC reporters, Dear Drabosa and Jasmine Woo, they're not engineers, right? They're reporters. They sat down with Claude Co-work and they asked it to build them a Monday.com replacement. That's the project management tool, right? A project management dashboard that had calendar views. It had email integration, task boards, team coordination features. This is the product that monday.com has spent years and hundreds of millions of dollars building. It s currently supports a $5 billion market cap for monday.com. It took these reporters under an hour. Total compute cost somewhere between $5 and $15. I hasten to add that is not the same thing as rebuilding monday.com. This was personal software. It's not deployed. It's not for sale. It was just for them. So yes, it is a big deal. Yes, it is a generational change in our ability for nontechnical people to make software. No, I am not saying that dear Drebosa and Jasmine Woo can refound monday.com for $10. The real story is that AI can build the tools you use, the software you pay per seats for, the dashboards your company spent 6 months specking out with a vendor, an AI agent can build a working version of that in an afternoon, and you don't need to write a line of code to make it happen. Yes, it might just be for you. It's a whole new category, people. It's called personal software. It didn't exist just a few months ago. It is now increasingly easy to make that happen. Our daily experience with AI is changing in ways that are really difficult to benchmark, but that doesn't mean that they're not structural. A marketing ops team using Claude Co-work can do content audits in just a few minutes instead of hours and hours. A finance analyst running due diligence doesn't take a day to do it because the model can read the document set, identify the risks, and produce lawyer ready redlines in just a few minutes. Our rhythm of work is different now. We can dispatch five different tasks in a few minutes on Claude Co-work. We can dispatch a PowerPoint deck, a financial model, a research synthesis, two data analyses, right? walk away, you can grab a cup of coffee, and you can come back and the deliverables are just done. They're not drafts anymore. It's just finished work. It's mostly formatted, right? The pattern that's emerging for non-technical users is what anthropic Scott White calls quote vibe working. You describe the outcomes, not the process. You don't you don't tell the AI how to build the spreadsheet. You tell it what the spreadsheet needs to show. It figures out the formulas. It figures out the formatting. It figures out the data connections. The shift is coming for all of us and it's going from operating tools to directing agents. And the skill that matters now is not technical proficiency. It's clarity of our intent. Knowing what you want, being able to articulate the real requirement, not just your surface request. That is becoming the bottleneck. Ironically, it's the same bottleneck the developers are hitting, but from a different direction. The C compiler agents didn't need anyone to write code for them. They needed someone to specify what a C compiler means precisely enough that 16 agents could coordinate on building one. The marketing team doesn't need someone to operate their analytics platform anymore. They need someone who knows which metrics matter and can explain why. The leverage across the board has shifted from execution to judgment across every function. Whether you write code or not, if you lead an organization, the number that should restructure your planning isn't measured in weeks or days. It's actually measured in revenue per employee. Purser, the AI coding tool hit $und00 million in annual recurring revenue with about 20 people. That's $5 million per employee. Midjourney generated 200 million people. Midjourney generated $200 million with about 40 people. Lovable, the AI app builder, they reached $200 million in 8 months with 15 people. For traditional SAS companies, $300,000 in revenue per employee is considered excellent and $600,000 is considered elite. That would be notion. AI native companies are running at five to seven times that number. Not because they found better people necessarily, but because their people orchestrate agents instead of doing the execution themselves. McKenzie published a framework not for others, but for themselves last month. They're targeting parody, matching the number of AI agents at McKenzie to human workers across the firm by the end of 2026. This is the company that sells organizational design to every Fortune 500 on Earth. And they're saying the org chart is about to flip. The pattern is visible at startups, too. Jacob Bank. The pattern is visible at startups, too. Jacob Bank runs a million-doll marketing operation with zero employees and roughly 40 AI agents. Micro One conducts 3,000 AI powered interviews every single day, handled at a tiny fraction of the headcount that enterprise recruiting firms need floors of people to do. Three developers in London built a complete business banking platform in 6 months. A project that would have required 20 engineers in 18 months before AI. Amazon's famous two pizza team formula, the idea that no team should be larger than what two pizzas can feed, is evolving into something even smaller. The emerging model at startups is now two to three humans plus a fleet of specialized agents all organized not by function but by outcome. The humans regardless of what their job title says and it increasingly doesn't matter set direction, evaluate quality and make judgment calls. The agents execute, coordinate, and scale. The org chart stops being a hierarchy of people and it becomes a map of human agent teams each owning a complete workflow end to end. For leaders, this changes the fundamental equation we've been working with for a long time. It's not about how many people do we need to hire now. It's about how many agents per person is the right ratio and what does each person need to be really excellent at to make that ratio work. The answer to the second question is really the same thing that's distinguished really excellent people for a long time in software. It's great judgment. It's what we call taste, which is vague, but typically means deeply understanding what the customer wants and being able to build it. It's about domain expertise. It's the about the ability to know whether the output is actually really, really good. And those skills now have 100 orex leverage because they are multiplied by the number of agents that person can direct and drive against. out. Daario Amade, anthropic CEO, has set the odds on a billion dollar solo founded company emerging by the end of 2026 at between 70 and 80%. Think about it. He thinks there's a 75% chance that there will be a billion dollar solo founded company by the end of this year. Sam Alman apparently has a betting pool among tech CEOs on the exact same question. Now, whether or not you believe that version, the direction is undeniable. The relationship between headcount and output is broken. And the organizations that figure out the new ratio first are going to outrun everybody who is still assuming they need dozens of developers to do one major software project. If you follow the trajectory that Opus 4.6 set, by 2026, June, July, August, somewhere in there, I would expect agents working autonomously for weeks to become routine rather than remarkable. By the end of the year, we are likely to see agents building full applications over potentially a month or more at a time. Not toy applications, real production systems with real architecture decisions complete with security reviews, with test suites, with documentation, all handled by agent teams. The trajectory from hours to two weeks took just 3 months. The trajectory from two weeks to months, that's coming soon. And the inference demand that this generates agents consuming tokens continuously around the clock across thousands of parallel sessions. Companies are not ready for this. This is what makes the $650 billion in hypers scale infrastructure look conservative rather than insane. Those data centers are not being built for chatbots people. They're being built for agent swarms running at a scale that people have had difficulty modeling or wrapping their heads around. Opus 4.6 gives us a sense of that future. So, what can you do about it? If you're sitting here thinking, "Oh my gosh, this is too much. It is coming too fast." You're not alone. You're not alone. You can do this. If you write code, try running a multi- aent session on real work, not a toy problem, a piece of your codebase with real technical depth. Watch how the agents coordinate. That experience is going to change your mental model of what agents can do in a way that matters much more than anything else. Because increasingly the way we work is the bottleneck for AI. If we want to go faster and build more, if we want to feel like we have the ability to do production work at the speed that AI demands because increasingly that's what they're going to expect from humans. I got to say, if we want to be ready for the future, best way we can do it is to change our mental models. If you don't write code, open up Claude Co-work. handed a task you've been procrastinating on that's that's felt really hard, right? A competitive analysis task, maybe a financial model task, a content audit across last quarter's output. Just describe the outcome you want, not the steps to get there. See what comes back. The gap between what you expect and what you get is the gap between your current mental model and where the tools are today. And for managers, look honestly at the 20 hours a week your team spends on operational coordination, ticket routing, dependency tracking. ask how many of those hours really require excellent human judgment and which are just pattern matching because I got to say your AI can probably take over a lot of the coordination work already and if you run an organization if you're on the senior leadership team you got to understand the question for your org has changed it's not about should we adopt AI or even which teams adopt it first it's really what is our agentto human ratio and what does each human need to be excellent at to make that ratio work and how do We support our humans to get there. The people working in knowledge work desperately need their leaders to understand that humans need a ton of support to get through this change management and become a new kind of worker that partners with AI. That is not an easy thing and most orgs are underinvesting in their people. I tell people in AI that if you are on the cutting edge of AI, it feels like you're time traveling always because you look at what's happening around you and then you go and you talk to people who haven't heard about it and they look at you like you're crazy. They say, "No, you can't do that. You can't run 16 agents at a time and run a Rust compiler. What do you mean?" And AI can manage 50 people. And when you tell them that's just a Wednesday in February and more is coming soon, then they really roll their eyes. But welcome to February. This is where we are. AI agents can build production grade compilers in just two weeks. They manage engineering orgs autonomously. They can discover hundreds of security vulnerabilities that human researchers missed. They can build your competitor's product in an hour for the cost of your lunch. They can coordinate in teams, resolve conflicts, and deliver at a level that did not exist 8 weeks ago. None of this was possible in January. And we don't know where this stops. We just know it's going faster. That's the tension underneath all of the benchmark scores, all the deployment numbers. The fact is the agents are here, they work, and it's just getting faster from here. And we're not sure what happens next. The question I have for all of us is how do we do a better job supporting each other in adjusting to what feels like a race out of MadMax some days? Welcome to February. It's moving fast. If you're a people leader, you need to take time to think about how to support your people to make it through this transition. If you are an individual contributor or a manager, I am putting as many tools as I can up on the Substack to help you get through this. But the best thing you can do, it's not about the Substack. I don't care. It's about you touching the AI and getting hands-on and actually building or trying to build with an AI agent system that launched not in January, not in December, but in February. And you need to take that mindset forward every single month. In March, you should be touching an AI system that was built in March. Every month now matters. Make sure that you don't miss it because our future as knowledge workers increasingly depends on our ability to keep the pace and work with AI agents.